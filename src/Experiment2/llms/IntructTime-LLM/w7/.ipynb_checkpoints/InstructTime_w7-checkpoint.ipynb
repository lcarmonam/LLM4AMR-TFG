{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78146ed-3a73-4a4e-a523-c82006d33053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import random, os, json\n",
    "\n",
    "import random, os, json\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from transformers import LlamaConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import gc\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbac23-0518-43f7-a0c5-bd5b11031110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebb022-4530-4405-991e-e7201963a24c",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8519db9-911a-4357-9ea2-7cdd6f032954",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **batch_size**: Number of samples per batch used during training.\n",
    "- **dropout**: Dropout rate applied to prevent overfitting.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization.\n",
    "- **lr**: Learning rate assigned to the optimizer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375e642-91e8-4422-b5ab-de55cc5a335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]\n",
    "batch_size = 4\n",
    "n_epochs_max = 100\n",
    "\n",
    "hyperparameters = {\n",
    "    \"batch_size\": batch_size,          \n",
    "    \"n_epochs_max\": n_epochs_max,                \n",
    "    \"weight_decay\": 1e-4,  \n",
    "    \"lr\": 5e-5, \n",
    "    \"patience\": 15, \n",
    "    \"dropout\": 0.15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907d6b8-2138-4d76-b057-70dcfbcbe03b",
   "metadata": {},
   "source": [
    "### FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6b352-dd7e-4998-9592-5fe544df530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        all_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(all_dims[i], all_dims[i + 1]) for i in range(len(all_dims) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.linear_layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.linear_layers) - 1:\n",
    "                x = F.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InstructTime(nn.Module): \n",
    "    def __init__(self, config, input_dim=87, time_steps=7, hidden_dim=256, dropout=0.15):\n",
    "        super().__init__()  \n",
    "\n",
    "        config.output_hidden_states = True \n",
    "        llama_model = LlamaForCausalLM(config)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,  \n",
    "            lora_alpha=32, \n",
    "            lora_dropout=0.1, \n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  \n",
    "        )\n",
    "        self.model = get_peft_model(llama_model, lora_config)  \n",
    "\n",
    "        self.temporal_projection = MLP(input_dim, [64, 128, hidden_dim], config.hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_size, nhead=config.num_attention_heads, dim_feedforward=hidden_dim, dropout=dropout  \n",
    "        )\n",
    "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.temporal_projection(x)         \n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.temporal_encoder(x)  \n",
    "        x = x.permute(1, 0, 2)  \n",
    "\n",
    "        llama_outputs = self.model(inputs_embeds=x, output_hidden_states=True) \n",
    "        hidden_states = llama_outputs.hidden_states[-1] \n",
    "\n",
    "        x = hidden_states[:, -1, :]\n",
    "        output = self.classifier(x).squeeze(-1) \n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf60b60-b98d-4807-9313-eae8b36653a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001, seed=42, verbose=False):\n",
    "    \"\"\"\n",
    "    Early stops the training if validation ROC AUC does not improve after a given patience.\n",
    "\n",
    "    Args:\n",
    "        - patience: Number of epochs to wait for an improvement before stopping the training. \n",
    "        - delta: Minimum change in the monitored metric to qualify as an improvement.\n",
    "        - verbose: If True, prints detailed messages each time the model improves and when early stopping is triggered.\n",
    "    \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_roc_auc = None  \n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, roc_auc, model):\n",
    "        if self.best_roc_auc is None or roc_auc > self.best_roc_auc + self.delta:\n",
    "            self.save_checkpoint(roc_auc, model)\n",
    "            self.best_roc_auc = roc_auc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, roc_auc, model):\n",
    "        if self.verbose:\n",
    "            print(f\"ROC AUC improved ({self.best_roc_auc if self.best_roc_auc is not None else 0:.4f} --> {roc_auc:.4f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), f'checkpoint_{seed}.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a5b55-082b-4e71-9d91-17d236c2f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    \n",
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed): \n",
    "    \"\"\"\n",
    "    Trains and evaluates the built model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        - model: The trained PyTorch model.\n",
    "        - history: Training history object containing loss and metrics.\n",
    "    \"\"\"  \n",
    "    device = torch.device('cuda:1')\n",
    "    batch_size=hyperparameters['batch_size']\n",
    "    \n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    config = LlamaConfig.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    model = InstructTime(config, dropout=hyperparameters['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr'], weight_decay=hyperparameters['weight_decay'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_auc_history = [] \n",
    "    early_stopping = EarlyStopping(patience=hyperparameters['patience'], delta=0.001, seed=seed, verbose=True)\n",
    "\n",
    "\n",
    "    for epoch in range(hyperparameters['n_epochs_max']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch).view(-1)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_val_preds = []\n",
    "        y_val_true = []\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "                val_logits = model(X_val_batch).view(-1)\n",
    "                loss = criterion(val_logits, y_val_batch)\n",
    "                val_loss += loss.item()\n",
    "                probs = torch.sigmoid(val_logits)\n",
    "\n",
    "                y_val_preds.extend(probs.cpu().numpy())\n",
    "                y_val_true.extend(y_val_batch.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        y_val_preds = np.array(y_val_preds) \n",
    "        y_val_true = np.array(y_val_true)\n",
    "        accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp  = calculate_metrics(y_val_true, y_val_preds)\n",
    "        \n",
    "        val_auc_history.append(roc_auc)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Metrics - Accuracy: {accuracy:.4f}, f1: {f1:.4f}, sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, ROC AUC: {roc_auc:.4f} \")\n",
    "        print(f\"Confusion Matrix - TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "\n",
    "        early_stopping(roc_auc, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break  \n",
    "    \n",
    "\n",
    "    model.load_state_dict(torch.load(f'checkpoint_{seed}.pt')) \n",
    "\n",
    "    history = {'loss': train_loss_history, 'val_loss': val_loss_history,'val_auc': val_auc_history}\n",
    "    return model, history  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b17259-0674-4ac8-84e1-ba8018b64f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "   \n",
    "    Returns:\n",
    "        - metric_dev: Best validation AUC achieved during training.     \n",
    "    \"\"\"\n",
    "    trial_seed = seed + trial.number  \n",
    "    set_seed(trial_seed)\n",
    "\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy['lr'] = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    hyperparameters_copy['dropout'] = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int(\"patience\", 3, 20)\n",
    "    \n",
    "    v_val_auc = [] \n",
    "    \n",
    "    model, history = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            trial_seed, \n",
    "        )\n",
    "\n",
    "    v_val_auc.append(max(history['val_auc']))\n",
    "    metric_dev = np.mean(v_val_auc)\n",
    "\n",
    "    return metric_dev\n",
    "\n",
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"   \n",
    "    set_seed(seed)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val),\n",
    "        n_trials=20,  \n",
    "        n_jobs=1      \n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "        'lr': best_params['lr'], \n",
    "        'weight_decay': best_params['weight_decay'],   \n",
    "        'dropout': best_params['dropout'], \n",
    "        'patience': best_params['patience'],  \n",
    "    }\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b1899-b904-410d-bfef-b6ba1c0ea41e",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad5428-c478-42aa-a2b9-7839b7eb064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "run_model = True\n",
    "results = []\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "\n",
    "    y_pred_by_split = {}\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    \n",
    "    \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w7days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w7days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w7days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val\n",
    "        )\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "        \n",
    "        split_directory = f'./Results_InstructTimeLlama/split_{i}'\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            'lr': bestHyperparameters['lr'], \n",
    "            'weight_decay': bestHyperparameters['weight_decay'],\n",
    "            'dropout': bestHyperparameters['dropout'], \n",
    "            'patience': bestHyperparameters['patience'], \n",
    "        })\n",
    "        \n",
    "        model, history = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(history['loss'])\n",
    "        loss_dev.append(history['val_loss'])\n",
    "        loss_dev.append(history['val_auc'])\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                logits = model(X_batch).view(-1)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                test_loss += loss.item()\n",
    "                probs = torch.sigmoid(logits)\n",
    "                        \n",
    "                y_pred.extend(probs.cpu().numpy().flatten())\n",
    "                y_true.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        y_pred = np.array(y_pred)  \n",
    "        y_true = np.array(y_true)\n",
    "        \n",
    "        accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "        print(f\" Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Metrics for Split {i} - Accuracy: {accuracy:.4f}, f1: {f1:.4f}, sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix - TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "\n",
    "        results.append([accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp])\n",
    "\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        print(f\"for split {i}:\")\n",
    "        print(y_pred_by_split[str(i)])\n",
    "\n",
    "        y_pred_path = os.path.join(split_directory, f\"y_pred_split_{i}.pkl\")\n",
    "        with open(y_pred_path, 'wb') as f:\n",
    "            pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc8f31-f01f-402f-9485-d62f37912b25",
   "metadata": {},
   "source": [
    "### RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd7562-9896-4f0d-a735-e616c768fd3a",
   "metadata": {},
   "source": [
    "#### Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168f65d-db46-4976-a481-58b5edd7775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_InstructTimeLlama'\n",
    "\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_InstructTimeLlama/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_InstructTimeLlama/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_InstructTimeLlama/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce692c0-5f5c-4d41-b636-89b585624531",
   "metadata": {},
   "source": [
    "#### Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea743d-2eda-4b85-9f1c-80bc55cf16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]: \n",
    "    y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cf7bc-60ae-48f8-9e7b-0080cf2d9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_InstructTimeLlama = pd.concat(all_metrics)\n",
    "metrics_InstructTimeLlama.to_csv('./Results_InstructTimeLlama/metrics_InstructTimeLlama.csv', index=False)\n",
    "\n",
    "metrics_InstructTimeLlama.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502ede9-6077-4233-8216-95cf6b4e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_InstructTimeLlama.mean()\n",
    "metrics_std = metrics_InstructTimeLlama.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "summary_df.to_csv('./Results_InstructTimeLlama/metrics_summary_InstructTimeLlama.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf060b0c-a2aa-4df8-ace8-90ea79f4c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_InstructTime = pd.read_csv('./Results_InstructTimeLlama/metrics_InstructTimeLlama.csv')\n",
    "stats_InstructTime= metrics_InstructTime.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_InstructTime.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_InstructTimeLlama/metrics_InstructTime_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a861d1-c681-4f23-833b-aeffe3d5d7fb",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7e96a-c13c-4680-90fa-4a4c90860674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        all_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(all_dims[i], all_dims[i + 1]) for i in range(len(all_dims) - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.linear_layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.linear_layers) - 1:\n",
    "                x = F.gelu(x)\n",
    "        return x\n",
    "\n",
    "class InstructTime(nn.Module):\n",
    "    def __init__(self, config, input_dim=87, time_steps=7, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        config.output_hidden_states = True\n",
    "        llama_model = LlamaForCausalLM(config)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "        )\n",
    "        self.model = get_peft_model(llama_model, lora_config)\n",
    "        self.temporal_projection = MLP(input_dim, [64, 128, hidden_dim], config.hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_size,\n",
    "            nhead=config.num_attention_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.15\n",
    "        )\n",
    "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_projection(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.temporal_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        llama_outputs = self.model(inputs_embeds=x, output_hidden_states=True)\n",
    "        hidden_states = llama_outputs.hidden_states[-1]\n",
    "        x = hidden_states[:, -1, :]\n",
    "        return self.classifier(x)\n",
    "        \n",
    "\n",
    "feature_names = [\n",
    "    'AMG', 'ATF', 'ATI', 'ATP', 'CAR', 'CF1', 'CF2', 'CF3', 'CF4', 'Falta', \n",
    "    'GCC', 'GLI', 'LIN', 'LIP', 'MAC', 'MON', 'NTI', \n",
    "    'OTR', 'OXA', 'PAP', 'PEN', 'POL', 'QUI', \n",
    "    'SUL', 'TTC', 'hoursVM', 'acinet.$_{pc}$', 'enterobac.$_{pc}$', 'enteroc.$_{pc}$',\n",
    "    'pseud.$_{pc}$', 'staph.$_{pc}$', 'others.$_{pc}$', 'hoursICU', '# pat_atb', '# pat_MR', \n",
    "    'CAR.$_{n}$', 'PAP.$_{n}$', 'Falta.$_{n}$', 'QUI.$_{n}$', \n",
    "    'ATF.$_{n}$', 'OXA.$_{n}$', 'PEN.$_{n}$', 'CF3.$_{n}$', \n",
    "    'GLI.$_{n}$', 'CF4.$_{n}$', 'SUL.$_{n}$', 'NTI.$_{n}$', \n",
    "    'LIN.$_{n}$', 'AMG.$_{n}$', 'MAC.$_{n}$', 'CF1.$_{n}$', 'GCC.$_{n}$', \n",
    "    'POL.$_{n}$', 'ATI.$_{n}$', 'MON.$_{n}$', 'LIP.$_{n}$', 'TTC.$_{n}$', \n",
    "    'OTR.$_{n}$', 'CF2.$_{n}$', 'ATP.$_{n}$', '# pat_ttl', 'posture.$_{change}$', \n",
    "    'insulin', 'nutr_art', 'sedation', 'relax', 'hep_fail', 'renal_fail', \n",
    "    'coag_fail', 'hemo_fail', 'resp_fail', 'multi_fail', 'n_transf', \n",
    "    'vasoactive.$_{drug}$', 'dosis_nems', 'hoursTracheo', 'hoursUlcer', \n",
    "    'hoursHemo', 'C01 PIVC 1', 'C01 PIVC 2', 'C02 CVC - YD', 'C02 CVC - SD', \n",
    "    'C02 CVC - SI', 'C02 CVC - FD', 'C02 CVC - YI', 'C02 CVC - FI', '# catheters'\n",
    "]\n",
    "\n",
    "all_shap_values = []\n",
    "all_inputs = []\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    print(f\"Processing split {i}...\")\n",
    "    seed = seeds[i-1]\n",
    "    model_path = f\"./checkpoint_{seed}.pt\"\n",
    "    if os.path.exists(model_path):\n",
    "        config = LlamaConfig.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "        model = InstructTime(config).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Model loaded.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model not found in {model_path}\")\n",
    "    \n",
    "    X_test = np.load(f\"../../../DATA/w7days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "    X_test_tensor = torch.tensor(np.where(X_test == 666, 0, X_test), dtype=torch.float32).to(device)\n",
    "    background = X_test_tensor\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "    shap_values = explainer.shap_values(X_test_tensor)\n",
    "    all_shap_values.append(shap_values)\n",
    "    all_inputs.append(X_test_tensor.cpu().numpy())\n",
    "\n",
    "all_shap_concat = np.concatenate(all_shap_values, axis=0)\n",
    "all_inputs_concat = np.concatenate(all_inputs, axis=0)\n",
    "\n",
    "shap_values_avg = all_shap_concat.mean(axis=1)\n",
    "X_test_avg = all_inputs_concat.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_avg,\n",
    "    X_test_avg,\n",
    "    feature_names=feature_names,\n",
    "    max_display=30,\n",
    "    show=False \n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xlabel(\"\")  \n",
    "plt.ylabel(\"\") \n",
    "\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.tick_params(labelsize=20)\n",
    "cbar.set_ylabel(\"Feature value\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_instructtime7.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96837ae-eb90-48e1-9027-4c1f89301e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(18, 12)) \n",
    "shap_values = shap.Explanation(values=shap_values_avg, \n",
    "                               data=X_test_avg,  \n",
    "                               feature_names=feature_names)\n",
    "\n",
    "shap.plots.heatmap(shap_values, show=False)\n",
    "\n",
    "plt.xticks(fontsize=25, rotation=45)\n",
    "plt.yticks(fontsize=25)\n",
    "ax = plt.gca()\n",
    "yticklabels = [label.get_text() for label in ax.get_yticklabels()]\n",
    "yticklabels = [\"Remaining\" if \"Sum of\" in label else label for label in yticklabels]\n",
    "ax.set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "plt.xlabel(\"\")  \n",
    "plt.ylabel(\"\") \n",
    "\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.tick_params(labelsize=19)\n",
    "cbar.set_ylabel(\"\")  \n",
    "\n",
    "plt.savefig(\"shap_heatmap_instruct7.png\", dpi=300, bbox_inches='tight')  \n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
