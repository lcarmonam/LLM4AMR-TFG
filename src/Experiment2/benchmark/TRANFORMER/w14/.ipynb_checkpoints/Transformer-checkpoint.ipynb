{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Dropout, Dense, Dropout, Flatten, Conv1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import utils_models \n",
    "import utils_interpretability\n",
    "import utils\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb24add-3925-44e8-b297-109f0ccd6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy():\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - y_true: A tensor containing the true labels (0 or 1).\n",
    "            - y_pred: A tensor containing the predicted probabilities for the positive class.\n",
    "        Returns:\n",
    "            Binary cross-entropy loss computed by TensorFlow's BinaryCrossentropy, \n",
    "            ignoring invalid labels.\n",
    "        \"\"\"\n",
    "        mask = tf.not_equal(y_true, 666) \n",
    "        y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "        y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "        \n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "        return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e973f-b30a-4735-9c02-646bceebac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b658ca",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a Transformer model based on the provided training data and hyperparameters.\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the model hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CONSIDER THE HYPERPARAMETERS\n",
    "    hyperparameters['layers'] = [87, hyperparameters['middle_layer_dim'], 1]\n",
    "    dropout = hyperparameters[\"dropout\"]\n",
    "    num_heads = hyperparameters[\"num_heads\"]\n",
    "    num_transformer_blocks = hyperparameters[\"num_transformer_blocks\"]\n",
    "    activation = hyperparameters['activation']\n",
    "    l2_reg = hyperparameters.get('l2_reg',  1e-4)  # Add L2 regularization strength\n",
    "    optimizer = Adam(learning_rate=hyperparameters[\"lr_scheduler\"], weight_decay=hyperparameters[\"weight_decay\"])\n",
    "\n",
    "\n",
    "    input = Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    x = input\n",
    "    masked = x\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # NORMALIZATION AND ATTENTION\n",
    "        x_norm = layers.LayerNormalization()(masked)\n",
    "        x_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input.shape[-1])(x_norm, x_norm)\n",
    "        x_att_drop = layers.Dropout(dropout)(x_att)\n",
    "        res = x_att_drop + masked\n",
    "\n",
    "        # FEED FORWARD PART\n",
    "        x_ffn_norm = layers.LayerNormalization()(res)\n",
    "        x_ffn = layers.Dense(\n",
    "            input.shape[-1], activation=activation, kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(x_ffn_norm)\n",
    "        x = x_ffn + res\n",
    "\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(\n",
    "        hyperparameters[\"layers\"][1], activation=activation, kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    output = layers.GlobalMaxPooling1D()(x)  \n",
    "    output = layers.Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(l2_reg))(output)  # Output: (None, 1)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    # COMPILE \n",
    "    model.compile(\n",
    "        loss=binary_crossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', \"AUC\"], weighted_metrics = [] \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, \n",
    "                hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built Transformer model based on the provided data and hyperparameters.\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.\n",
    "    Returns:\n",
    "        - model (tf.keras.Model): The trained Keras model.\n",
    "        - hist (tf.keras.callbacks.History): Training history object containing loss and metrics.\n",
    "        - training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the transformer\n",
    "    model = None\n",
    "    model = build_model(hyperparameters) \n",
    "\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_auc\",\n",
    "        min_delta=hyperparameters[\"mindelta\"],\n",
    "        patience=hyperparameters[\"patience\"],\n",
    "        restore_best_weights=True,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "    \n",
    "    X_val = tf.cast(X_val, tf.float64)\n",
    "    y_val = tf.cast(y_val, tf.float64)\n",
    "    X_train = tf.cast(X_train, tf.float64)\n",
    "    y_train = tf.cast(y_train, tf.float64)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopping],\n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        epochs=hyperparameters['n_epochs_max'],\n",
    "        verbose=hyperparameters[\"verbose\"],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, hist, training_time\n",
    "\n",
    "\n",
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.    \n",
    "    Returns:\n",
    "        - metric_dev (float): Best validation AUC achieved during training.     \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Trial {trial.number} started\")\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    middle_dim = trial.suggest_int('middle_layer_dim', 2, 20, step=2)\n",
    "    hyperparameters_copy['middle_layer_dim'] = middle_dim\n",
    "    hyperparameters_copy[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hyperparameters_copy['l2_reg'] = trial.suggest_loguniform('l2_reg', 1e-6, 1e-2)\n",
    "    hyperparameters_copy['num_transformer_blocks'] = trial.suggest_int(\"num_transformer_blocks\", 1, 5)\n",
    "    hyperparameters_copy['activation'] = trial.suggest_categorical(\"activation\", ['tanh', 'LeakyReLU'])\n",
    "    hyperparameters_copy['num_heads'] = trial.suggest_int(\"num_heads\", 2, 10)\n",
    "    hyperparameters_copy['epsilon'] = trial.suggest_categorical(\"epsilon\", [0.9, 0.5, 0.1])\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int('patience', 3, 20)\n",
    "    hyperparameters_copy['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-5)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "\n",
    "    hyperparameters_copy['batch_size'] = hyperparameters['batch_size']\n",
    "    hyperparameters_copy['n_epochs_max'] = hyperparameters['n_epochs_max']\n",
    "    \n",
    "    v_val_auc = []\n",
    "\n",
    "    model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "    )\n",
    "\n",
    "    v_val_auc.append(np.max(hist.history[\"val_auc\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_auc)\n",
    "    return metric_dev\n",
    "\n",
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.       \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize') \n",
    "    study.optimize(lambda trial: objective(trial, hyperparameters, seed, X_train, y_train , X_val, y_val, split, norm, n_time_steps), n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    layers = [87, best_params['middle_layer_dim'], 1]\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "                'dropout': best_params['dropout'],\n",
    "                'middle_layer_dim': best_params['middle_layer_dim'],\n",
    "                'layers': layers,\n",
    "                'lr_scheduler': best_params['lr_scheduler'],\n",
    "                'l2_reg': best_params['l2_reg'],\n",
    "                'num_transformer_blocks': best_params['num_transformer_blocks'],\n",
    "                'activation': best_params['activation'],\n",
    "                'num_heads': best_params['num_heads'],\n",
    "                'epsilon': best_params['epsilon'],\n",
    "                'batch_size': hyperparameters['batch_size'],\n",
    "                'n_epochs_max': hyperparameters['n_epochs_max'],\n",
    "                'patience': best_params['patience'],\n",
    "                'mindelta': best_params['mindelta'],\n",
    "        \n",
    "                'weight_decay': best_params['weight_decay']\n",
    "            }\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "    \n",
    "    return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447021ef",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3c500",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **n_epochs_max**: Maximum number of epochs for training.\n",
    "- **l2_reg**: L2 regularization coefficient.\n",
    "- **dropout**: Dropout rates.\n",
    "- **lr_scheduler**: Learning rates.\n",
    "- **norm**: Type of normalization applied to the data.\n",
    "- **num_heads**: Number of attention heads in the multi-head attention mechanism.\n",
    "- **num_transformer_blocks**: Number of transformer blocks.\n",
    "- **epsilon**: Avoid zero division in the normalization layer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization on weights.\n",
    "- **middle_layer_dim**: Different configurations for the middle layer of the model.\n",
    "- **mindelta**: Minimum delta required to consider as an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]\n",
    "\n",
    "input_shape = 87\n",
    "n_time_steps = 14\n",
    "batch_size = 4\n",
    "n_epochs_max = 100\n",
    "\n",
    "\n",
    "adjustment_factor = [1] \n",
    "activation = ['tanh', 'LeakyReLU']\n",
    "norm = \"standardScaler\"\n",
    "patience = 3\n",
    "monitor = \"val_auc\" \n",
    "\n",
    "num_heads = 7\n",
    "num_transformer_blocks = [5]\n",
    "epsilon = [0.9, 0.5, 0.1]\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": monitor,\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": patience,\n",
    "    \"dropout\": 0.2,\n",
    "    \"verbose\": 1,\n",
    "    \"input_shape\": input_shape,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"num_transformer_blocks\": 0,\n",
    "    \"l2_reg\": 1e-4,\n",
    "    \"epsilon\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766690a0",
   "metadata": {},
   "source": [
    "# RUNNING AND TRYING ON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    feature_names = [\n",
    "    'AMG', 'ATF', 'ATI', 'ATP', 'CAR', 'CF1', 'CF2', 'CF3', 'CF4', 'Falta', \n",
    "    'GCC', 'GLI', 'LIN', 'LIP', 'MAC', 'MON', 'NTI', \n",
    "    'OTR', 'OXA', 'PAP', 'PEN', 'POL', 'QUI', \n",
    "    'SUL', 'TTC', 'hoursVM', 'acinet.$_{pc}$', 'enterobac.$_{pc}$', 'enteroc.$_{pc}$',\n",
    "    'pseud.$_{pc}$', 'staph.$_{pc}$', 'others.$_{pc}$', 'hoursICU', '# pat_atb', '# pat_MR', \n",
    "    'CAR.$_{n}$', 'PAP.$_{n}$', 'Falta.$_{n}$', 'QUI.$_{n}$', \n",
    "    'ATF.$_{n}$', 'OXA.$_{n}$', 'PEN.$_{n}$', 'CF3.$_{n}$', \n",
    "    'GLI.$_{n}$', 'CF4.$_{n}$', 'SUL.$_{n}$', 'NTI.$_{n}$', \n",
    "    'LIN.$_{n}$', 'AMG.$_{n}$', 'MAC.$_{n}$', 'CF1.$_{n}$', 'GCC.$_{n}$', \n",
    "    'POL.$_{n}$', 'ATI.$_{n}$', 'MON.$_{n}$', 'LIP.$_{n}$', 'TTC.$_{n}$', \n",
    "    'OTR.$_{n}$', 'CF2.$_{n}$', 'ATP.$_{n}$', '# pat_ttl', 'posture.$_{change}$', \n",
    "    'insulin', 'nutr_art', 'sedation', 'relax', 'hep_fail', 'renal_fail', \n",
    "    'coag_fail', 'hemo_fail', 'resp_fail', 'multi_fail', 'n_transf', \n",
    "    'vasoactive.$_{drug}$', 'dosis_nems', 'hoursTracheo', 'hoursUlcer', \n",
    "    'hoursHemo', 'C01 PIVC 1', 'C01 PIVC 2', 'C02 CVC - YD', 'C02 CVC - SD', \n",
    "    'C02 CVC - SI', 'C02 CVC - FD', 'C02 CVC - YI', 'C02 CVC - FI', '# catheters'\n",
    "    ]\n",
    "\n",
    "    all_shap_values = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w14days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w14days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w14days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    "        \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "\n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val,\n",
    "            f\"s{i}\",\n",
    "            norm,\n",
    "            n_time_steps\n",
    "        )\n",
    "        \n",
    "        fin = time.time()\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_Transformer_optuna/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"], \n",
    "            \"l2_reg\": bestHyperparameters[\"l2_reg\"],\n",
    "            \"num_transformer_blocks\": bestHyperparameters[\"num_transformer_blocks\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"],\n",
    "            'num_heads': bestHyperparameters['num_heads'],\n",
    "            'epsilon': bestHyperparameters['epsilon'],\n",
    "            \"patience\": bestHyperparameters[\"patience\"], \n",
    "            \"weight_decay\": bestHyperparameters[\"weight_decay\"],\n",
    "            \"mindelta\": bestHyperparameters[\"mindelta\"],\n",
    "            \"middle_layer_dim\": bestHyperparameters[\"middle_layer_dim\"]\n",
    "        })\n",
    "\n",
    "        #--- TRY ON TEST -----------------------------------------------------------------------#\n",
    "\n",
    "        utils_models.reset_keras()\n",
    "\n",
    "        model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_auc'])\n",
    "        training_times.append(training_time)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "            \n",
    "        with open(os.path.join(split_directory, \"training_times.pkl\"), 'wb') as f:\n",
    "            pickle.dump(training_times, f)\n",
    "                    \n",
    "        # -------- SHAP --------\n",
    "        background = X_test[np.random.choice(X_test.shape[0], 100, replace=False)]\n",
    "        explainer = shap.GradientExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(X_test)[0]\n",
    "        \n",
    "        all_shap_values.append(shap_values)  \n",
    "        all_inputs.append(X_test)\n",
    "\n",
    "    all_shap_concat = np.concatenate(all_shap_values, axis=0)\n",
    "    all_inputs_concat = np.concatenate(all_inputs, axis=0)\n",
    "    \n",
    "    if all_shap_concat.ndim == 3:\n",
    "        shap_values_avg = all_shap_concat.mean(axis=1)\n",
    "        X_test_avg = all_inputs_concat.mean(axis=1)\n",
    "    else:\n",
    "        shap_values_avg = all_shap_concat\n",
    "        X_test_avg = all_inputs_concat\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values_avg,\n",
    "        X_test_avg,\n",
    "        feature_names=feature_names,\n",
    "        max_display=30,\n",
    "        show=False\n",
    "    )\n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    plt.yticks(fontsize=19)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    cbar.tick_params(labelsize=20)\n",
    "    cbar.set_ylabel(\"Feature value\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"shap_summary_transformer14.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap_obj = shap.Explanation(values=shap_values_avg, data=X_test_avg, feature_names=feature_names)\n",
    "    shap.plots.heatmap(shap_obj, show=False)\n",
    "    plt.xticks(rotation=45, fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"shap_heatmap_transformer14.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # END EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d440b",
   "metadata": {},
   "source": [
    "# RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b785b",
   "metadata": {},
   "source": [
    "## Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1692d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_Transformer_optuna'\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818650d",
   "metadata": {},
   "source": [
    "## Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d5ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]: \n",
    "    y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66867d5",
   "metadata": {},
   "source": [
    "## Save results (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer = pd.concat(all_metrics)\n",
    "metrics_Transformer.to_csv('./Results_Transformer_optuna/metrics_Transformer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b2397-f8df-4189-a9d8-e06c8f3cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f069d-664f-4d20-b251-44e776fe4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_Transformer.mean()\n",
    "metrics_std = metrics_Transformer.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "summary_df.to_csv('./Results_Transformer_optuna/metrics_summary_Transformer.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faecc5a-f631-4d47-b7ff-754dd55de734",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer = pd.read_csv('./Results_Transformer_optuna/metrics_Transformer.csv')\n",
    "stats_Transformer = metrics_Transformer.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_Transformer.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_Transformer_optuna/metrics_Transformer_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
