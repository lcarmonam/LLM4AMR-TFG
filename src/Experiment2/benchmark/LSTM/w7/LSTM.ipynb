{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31f2d8-461a-4b1b-b291-3c3398d5b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import utils_models \n",
    "import utils_interpretability\n",
    "import utils\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4faef-6d82-4384-86f5-f2c65d137158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy():\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - y_true: A tensor containing the true labels (0 or 1).\n",
    "            - y_pred: A tensor containing the predicted probabilities for the positive class.\n",
    "        Returns:\n",
    "            Binary cross-entropy loss computed by TensorFlow's BinaryCrossentropy, \n",
    "            ignoring invalid labels.\n",
    "        \"\"\"\n",
    "        mask = tf.not_equal(y_true, 666) \n",
    "        y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "        y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "        \n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "        return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951592a-81b2-4e3b-b456-9dcc8d395869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e72dea-acfc-45a8-b140-8a1521095f87",
   "metadata": {},
   "source": [
    "### FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f6163-a4d3-4304-af75-c4898fc64c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a LSTM model based on several hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the model hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    hyperparameters['layers'] = [87, hyperparameters['middle_layer_dim'], 1]\n",
    "    l2_lambda = hyperparameters.get(\"l2_lambda\", 1e-4)\n",
    "\n",
    "    \n",
    "    dynamic_input = tf.keras.layers.Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    masked = dynamic_input\n",
    "    optimizer = Adam(learning_rate=hyperparameters[\"lr_scheduler\"], weight_decay=hyperparameters[\"weight_decay\"])\n",
    "\n",
    "    lstm_encoder = tf.keras.layers.LSTM(\n",
    "        hyperparameters[\"layers\"][1],\n",
    "        dropout=hyperparameters['dropout'],\n",
    "        return_sequences=False,\n",
    "        activation=hyperparameters['activation'],\n",
    "        kernel_regularizer=regularizers.l2(l2_lambda),\n",
    "        use_bias=False\n",
    "    )(masked)\n",
    "    \n",
    "    if hyperparameters['dropout'] > 0.0:\n",
    "        lstm_encoder = tf.keras.layers.Dropout(hyperparameters['dropout'])(lstm_encoder)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, use_bias=False, activation=\"sigmoid\",kernel_regularizer=regularizers.l2(l2_lambda))(lstm_encoder)\n",
    "\n",
    "    model = tf.keras.Model(dynamic_input, output)\n",
    "    model.compile(\n",
    "        loss=binary_crossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', \"AUC\"], weighted_metrics = []\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5e399-a66f-4328-bc92-a8db8f5e33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(X_train, X_val, y_train, y_val, \n",
    "                hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built LSTM model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.\n",
    "    Returns:\n",
    "        - model (tf.keras.Model): The trained Keras model.\n",
    "        - hist (tf.keras.callbacks.History): Training history object containing loss and metrics.\n",
    "        - training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc',\n",
    "                                                  min_delta=hyperparameters[\"mindelta\"],\n",
    "                                                  patience=hyperparameters[\"patience\"],\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  mode=\"max\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     callbacks=[earlystopping], batch_size=hyperparameters['batch_size'], epochs=hyperparameters['n_epochs_max'],\n",
    "                     verbose=hyperparameters['verbose'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, hist, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea3a15-d2b1-4ed3-8e99-d5bdc1311db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.    \n",
    "    Returns:\n",
    "        - metric_dev (float): Best validation AUC achieved during training.     \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Trial {trial.number} started\")\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    middle_dim = trial.suggest_int('middle_layer_dim', 2, 20, step=2)\n",
    "    hyperparameters_copy['middle_layer_dim'] = middle_dim\n",
    "    hyperparameters_copy[\"l2_lambda\"] = trial.suggest_loguniform('l2_lambda', 1e-6, 1e-2)\n",
    "    hyperparameters_copy[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hyperparameters_copy[\"adjustment_factor\"] = trial.suggest_categorical('adjustment_factor', [1])\n",
    "    hyperparameters_copy[\"activation\"] = trial.suggest_categorical('activation', ['tanh', 'LeakyReLU'])\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int('patience', 3, 20)\n",
    "    hyperparameters_copy['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-5)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "    \n",
    "\n",
    "    hyperparameters_copy['batch_size'] = hyperparameters['batch_size']\n",
    "    hyperparameters_copy['n_epochs_max'] = hyperparameters['n_epochs_max']\n",
    "   \n",
    "    v_val_auc = []  \n",
    "\n",
    "    model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "    )\n",
    "\n",
    "    v_val_auc.append(np.max(hist.history[\"val_auc\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_auc)\n",
    "    return metric_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f881e-b237-4631-b9a8-185c6cc4a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.       \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='maximize') \n",
    "    study.optimize(lambda trial: objective(trial, hyperparameters, seed, X_train, y_train , X_val, y_val, split, norm, n_time_steps), n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    \n",
    "    layers = [87, best_params['middle_layer_dim'], 1]\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "        'dropout': best_params['dropout'],\n",
    "        'middle_layer_dim': best_params['middle_layer_dim'],\n",
    "        'layers': layers,\n",
    "        'lr_scheduler': best_params['lr_scheduler'],\n",
    "        'l2_lambda': best_params['l2_lambda'],\n",
    "        'adjustment_factor': best_params['adjustment_factor'],\n",
    "        'activation': best_params['activation'],\n",
    "        'batch_size': hyperparameters['batch_size'],\n",
    "        'n_epochs_max': hyperparameters['n_epochs_max'],\n",
    "        'patience': best_params['patience'],\n",
    "        'mindelta': best_params['mindelta'],\n",
    "        'weight_decay': best_params['weight_decay']\n",
    "    }\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "\n",
    "    return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3d036-d378-4070-9fca-3b054b8c8bfd",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad4025-45b5-49ea-8c9b-0d70464a3e1f",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **norm**: Type of normalization applied to the data.\n",
    "- **dropout**: Dropout rate to prevent overfitting.\n",
    "- **l2_lambda**: L2 regularization coefficient.\n",
    "- **lr_scheduler**: Learning rate assigned to the optimizer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization on weights.\n",
    "- **middle_layer_dim**: Different configurations for the middle layer of the model.\n",
    "- **mindelta**: Minimum delta required to consider as an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0a288-018e-4fad-bbf4-2be13135a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]\n",
    "\n",
    "input_shape = 87\n",
    "n_time_steps = 7\n",
    "batch_size = 4\n",
    "n_epochs_max = 100\n",
    "\n",
    "adjustment_factor = [1]  \n",
    "activation = ['tanh', 'LeakyReLU']\n",
    "norm = \"standardScaler\"\n",
    "patience = 3 \n",
    "monitor = \"val_auc\"   \n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": monitor,\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": patience,\n",
    "    \"dropout\": 0.2,\n",
    "    \"l2_lambda\": 1e-4,\n",
    "    \"verbose\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dc37e-323b-486b-a0b7-a62d3f6d79b4",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de524373-d8ca-4bd8-8c36-af823583fa2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    feature_names = [\n",
    "    'AMG', 'ATF', 'ATI', 'ATP', 'CAR', 'CF1', 'CF2', 'CF3', 'CF4', 'Falta', \n",
    "    'GCC', 'GLI', 'LIN', 'LIP', 'MAC', 'MON', 'NTI', \n",
    "    'OTR', 'OXA', 'PAP', 'PEN', 'POL', 'QUI', \n",
    "    'SUL', 'TTC', 'hoursVM', 'acinet.$_{pc}$', 'enterobac.$_{pc}$', 'enteroc.$_{pc}$',\n",
    "    'pseud.$_{pc}$', 'staph.$_{pc}$', 'others.$_{pc}$', 'hoursICU', '# pat_atb', '# pat_MR', \n",
    "    'CAR.$_{n}$', 'PAP.$_{n}$', 'Falta.$_{n}$', 'QUI.$_{n}$', \n",
    "    'ATF.$_{n}$', 'OXA.$_{n}$', 'PEN.$_{n}$', 'CF3.$_{n}$', \n",
    "    'GLI.$_{n}$', 'CF4.$_{n}$', 'SUL.$_{n}$', 'NTI.$_{n}$', \n",
    "    'LIN.$_{n}$', 'AMG.$_{n}$', 'MAC.$_{n}$', 'CF1.$_{n}$', 'GCC.$_{n}$', \n",
    "    'POL.$_{n}$', 'ATI.$_{n}$', 'MON.$_{n}$', 'LIP.$_{n}$', 'TTC.$_{n}$', \n",
    "    'OTR.$_{n}$', 'CF2.$_{n}$', 'ATP.$_{n}$', '# pat_ttl', 'posture.$_{change}$', \n",
    "    'insulin', 'nutr_art', 'sedation', 'relax', 'hep_fail', 'renal_fail', \n",
    "    'coag_fail', 'hemo_fail', 'resp_fail', 'multi_fail', 'n_transf', \n",
    "    'vasoactive.$_{drug}$', 'dosis_nems', 'hoursTracheo', 'hoursUlcer', \n",
    "    'hoursHemo', 'C01 PIVC 1', 'C01 PIVC 2', 'C02 CVC - YD', 'C02 CVC - SD', \n",
    "    'C02 CVC - SI', 'C02 CVC - FD', 'C02 CVC - YI', 'C02 CVC - FI', '# catheters'\n",
    "    ]\n",
    "\n",
    "    all_shap_values = []\n",
    "    all_inputs = []\n",
    "    \n",
    "        \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w7days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w7days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w7days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "        \n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val,\n",
    "            f\"s{i}\",\n",
    "            norm,\n",
    "            n_time_steps\n",
    "        )\n",
    "        print(f\"Best layers: {bestHyperparameters['layers']}\")\n",
    "        \n",
    "        fin = time.time()\n",
    "        \n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_LSTM_optuna/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"adjustment_factor\": bestHyperparameters[\"adjustment_factor\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"], \n",
    "            \"patience\": bestHyperparameters[\"patience\"], \n",
    "            \"weight_decay\": bestHyperparameters[\"weight_decay\"],\n",
    "            \"mindelta\": bestHyperparameters[\"mindelta\"],\n",
    "            \"l2_lambda\": bestHyperparameters[\"l2_lambda\"],\n",
    "            \"middle_layer_dim\": bestHyperparameters[\"middle_layer_dim\"]\n",
    "        })\n",
    "        \n",
    "        # --- TRY ON TEST -----------------------------------------------------------------------\n",
    "        utils_models.reset_keras()\n",
    "        print(hyperparameters)\n",
    "   \n",
    "\n",
    "        model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train, \n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_auc'])\n",
    "        training_times.append(training_time)\n",
    "\n",
    "        y_pred = model.predict(x=X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred \n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        with open(os.path.join(split_directory, \"training_times.pkl\"), 'wb') as f:\n",
    "            pickle.dump(training_times, f)\n",
    "            \n",
    "        # -------- SHAP --------\n",
    "        background = X_test[np.random.choice(X_test.shape[0], 100, replace=False)]\n",
    "        explainer = shap.GradientExplainer(model, background)\n",
    "        shap_values = explainer.shap_values(X_test)[0]\n",
    "        \n",
    "        all_shap_values.append(shap_values)  \n",
    "        all_inputs.append(X_test)\n",
    "\n",
    "    all_shap_concat = np.concatenate(all_shap_values, axis=0)\n",
    "    all_inputs_concat = np.concatenate(all_inputs, axis=0)\n",
    "    \n",
    "    if all_shap_concat.ndim == 3:\n",
    "        shap_values_avg = all_shap_concat.mean(axis=1)\n",
    "        X_test_avg = all_inputs_concat.mean(axis=1)\n",
    "    else:\n",
    "        shap_values_avg = all_shap_concat\n",
    "        X_test_avg = all_inputs_concat\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values_avg,\n",
    "        X_test_avg,\n",
    "        feature_names=feature_names,\n",
    "        max_display=30,\n",
    "        show=False\n",
    "    )\n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    plt.yticks(fontsize=19)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    cbar.tick_params(labelsize=20)\n",
    "    cbar.set_ylabel(\"Feature value\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"shap_summary_LSTM7.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap_obj = shap.Explanation(values=shap_values_avg, data=X_test_avg, feature_names=feature_names)\n",
    "    shap.plots.heatmap(shap_obj, show=False)\n",
    "    plt.xticks(rotation=45, fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"shap_heatmap_LSTM7.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # END EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5a323-fb8a-4699-8df8-37a7a3fb9147",
   "metadata": {},
   "source": [
    "### RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab83036-296f-41cf-94ae-911d7367c9d8",
   "metadata": {},
   "source": [
    "## Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e588296-98b8-4076-86f7-10727655e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_LSTM_optuna'\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    \n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_LSTM_optuna/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_LSTM_optuna/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_LSTM_optuna/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d10e9c-4090-4ad6-80cd-a3fd8ab8687c",
   "metadata": {},
   "source": [
    "## Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ad3a0-30e8-4bf1-ac5c-6e562fc5a143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]: \n",
    "    y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54945d6-b965-4023-a26d-a120ff2349ef",
   "metadata": {},
   "source": [
    "## Save results (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7606-a935-46a7-a27b-9eac995df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_LSTM = pd.concat(all_metrics)\n",
    "metrics_LSTM.to_csv('./Results_LSTM_optuna/metrics_LSTM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41e6ea-d570-456e-8e02-a71bff88b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_LSTM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7f362-b157-4c5a-8e94-7966ee8689ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_LSTM.mean()\n",
    "metrics_std = metrics_LSTM.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "summary_df.to_csv('./Results_LSTM_optuna/metrics_summary_LSTM.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54a150-4d1e-4a7c-a754-9c192c96c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_LSTM = pd.read_csv('./Results_LSTM_optuna/metrics_LSTM.csv')\n",
    "stats_LSTM = metrics_LSTM.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_LSTM.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_LSTM_optuna/metrics_LSTM_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
