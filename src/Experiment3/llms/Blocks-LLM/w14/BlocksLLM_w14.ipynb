{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38d76a-9e55-49c7-97c9-3d6b5e191c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import random, os, json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import recall_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d871fff-4bc7-4ec7-81a9-b6d53b124402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef54c8-d37e-4fad-b75f-dd52e435e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUCSensitivitySpecificityLoss(nn.Module):\n",
    "    def __init__(self, lambda_balance=1.0):\n",
    "        \"\"\"\n",
    "        Custom loss function that maximizes ROC-AUC and balances sensitivity and specificity.\n",
    "        Args:\n",
    "            lambda_balance (float): Weight for balancing sensitivity-specificity trade-off.\n",
    "        \"\"\"\n",
    "        super(AUCSensitivitySpecificityLoss, self).__init__()\n",
    "        self.lambda_balance = lambda_balance\n",
    "        self.bce_loss = nn.BCELoss()  \n",
    " \n",
    "    def forward(self, y_pred_probs, y_true):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        Args:\n",
    "            y_pred_probs (Tensor): Predicted probabilities (after Sigmoid).\n",
    "            y_true (Tensor): True binary labels (0 or 1).\n",
    "        Returns:\n",
    "            loss (Tensor): Computed loss value.\n",
    "        \"\"\"\n",
    "        bce = self.bce_loss(y_pred_probs, y_true)\n",
    "\n",
    " \n",
    "        y_pred_labels = (y_pred_probs > 0.5).float()  \n",
    "        tp = (y_pred_labels * y_true).sum()\n",
    "        tn = ((1 - y_pred_labels) * (1 - y_true)).sum()\n",
    "        fp = (y_pred_labels * (1 - y_true)).sum()\n",
    "        fn = ((1 - y_pred_labels) * y_true).sum()\n",
    " \n",
    "        sensitivity = tp / (tp + fn + 1e-7)  \n",
    "        specificity = tn / (tn + fp + 1e-7)  \n",
    " \n",
    "        balance_penalty = torch.abs(sensitivity - specificity).detach()\n",
    "        total_loss = bce * (1 + self.lambda_balance * balance_penalty)\n",
    "         \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c615ad-6398-4a78-8b56-3f6818e62c50",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aeef9-5239-4bb2-82de-f86596a826e7",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **batch_size**: Number of samples per batch used during training.\n",
    "- **dropout**: Dropout rate applied to prevent overfitting.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization.\n",
    "- **lr**: Learning rate assigned to the optimizer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered.\n",
    "- **max_tokens**: Maximum number of tokens allowed per block when tokenizing the input time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec9213-ff39-46c0-8638-d918293b667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]  \n",
    "batch_size = 32\n",
    "n_epochs_max = 100\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"batch_size\": batch_size,          \n",
    "    \"n_epochs_max\": n_epochs_max,     \n",
    "    \"lr\": 1e-5,            \n",
    "    \"weight_decay\": 1e-4,            \n",
    "    \"max_tokens\": 1024,\n",
    "    \"patience\": 15,\n",
    "    \"dropout\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c333d-a1b2-4c6e-933a-d7232503896f",
   "metadata": {},
   "source": [
    "### FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989c3a0-b9e4-48c4-92e9-39f6189777f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series_to_blocks(X, tokenizer, max_tokens=1024):\n",
    "    batch_size, time_steps, features = X.shape\n",
    "    text_blocks = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        blocks = []\n",
    "        current_block = []\n",
    "        current_tokens = 0\n",
    "\n",
    "        for i in range(time_steps):\n",
    "            timestep_text = \" \".join([f\"f{j}:{X[b, i, j]:.2f}\" for j in range(features)])\n",
    "            tokens = tokenizer(timestep_text, truncation=False)[\"input_ids\"]\n",
    "            token_length = len(tokens)\n",
    "\n",
    "            if current_tokens + token_length <= max_tokens:\n",
    "                current_block.append(timestep_text)\n",
    "                current_tokens += token_length\n",
    "            else:\n",
    "                if current_block:\n",
    "                    blocks.append(\" \".join(current_block))\n",
    "                current_block = [timestep_text]\n",
    "                current_tokens = token_length\n",
    "        if current_block:\n",
    "            blocks.append(\" \".join(current_block))\n",
    "\n",
    "        text_blocks.append(blocks)\n",
    "\n",
    "    return text_blocks\n",
    "\n",
    "class TimeSeriesGPT2WithBlocks(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=1, dropout=0.1):\n",
    "        super(TimeSeriesGPT2WithBlocks, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.model.config.hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, blocks, max_tokens=1024):\n",
    "        block_outputs = []\n",
    "\n",
    "        for block in blocks:\n",
    "            inputs = self.tokenizer(\n",
    "                block, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_tokens\n",
    "            )\n",
    "            inputs = {key: val.to(next(self.parameters()).device) for key, val in inputs.items()}\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.hidden_states[-1][:, -1, :]\n",
    "            block_outputs.append(cls_embedding)\n",
    "\n",
    "        combined_output = torch.mean(torch.stack(block_outputs), dim=0)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        logits = self.fc(combined_output)\n",
    "        return self.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35d661-bfa6-40ad-8692-a4dd50f278d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001, verbose=True, alpha=0.5, min_combined_metric=0.6, lambda_penalty=0.1):\n",
    "        \"\"\"\n",
    "        Early stops the training if validation ROC AUC does not improve after a given patience\n",
    "        and if Recall & Specificity are balanced.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.alpha = alpha\n",
    "        self.min_combined_metric = min_combined_metric\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.best_auc = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, roc_auc, recall, specificity, model):\n",
    "        metric_score = (self.alpha * recall) + ((1 - self.alpha) * specificity) - self.lambda_penalty * abs(recall - specificity)\n",
    "\n",
    "        if metric_score >= self.min_combined_metric:\n",
    "            if self.best_auc is None or roc_auc > self.best_auc + self.delta:\n",
    "                self.save_checkpoint(roc_auc, model)\n",
    "                self.best_auc = roc_auc\n",
    "                self.counter = 0 \n",
    "                print(f\"Model saved! Counter reset to 0.\")\n",
    "            else:\n",
    "                self.counter += 1 \n",
    "                print(f\"EarlyStopping counter increased: {self.counter} out of {self.patience}\")\n",
    "        else:\n",
    "            self.counter += 1 \n",
    "            print(f\"Model NOT saved. Counter increased: {self.counter} out of {self.patience}\")\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "    \n",
    "    def save_checkpoint(self, roc_auc, model):\n",
    "        if self.verbose:\n",
    "            print(f\"ROC AUC improved ({self.best_auc if self.best_auc is not None else 0:.4f} --> {roc_auc:.4f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a2ae1-db5f-43a3-b0f1-08d9634e78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed, log_file=\"training_log.txt\"):    \n",
    "    \"\"\"\n",
    "    Trains and evaluates the built model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.\n",
    "        - log_file: File path to save training logs.\n",
    "\n",
    "    Returns:\n",
    "        - model: The trained PyTorch model.\n",
    "        - history: Training history object containing loss and metrics.\n",
    "    \"\"\"      \n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "    set_seed(seed)\n",
    "    \n",
    "    model = TimeSeriesGPT2WithBlocks(\"distilgpt2\", dropout=hyperparameters['dropout']).to(device)\n",
    "    loss_fn = AUCSensitivitySpecificityLoss(lambda_balance=0.5)\n",
    "    optimizer = Adam(model.parameters(), lr=hyperparameters['lr'], weight_decay=hyperparameters['weight_decay'])\n",
    "\n",
    "    tokenizer = model.tokenizer\n",
    "    train_blocks = split_series_to_blocks(X_train, tokenizer, max_tokens=hyperparameters['max_tokens'])\n",
    "    val_blocks = split_series_to_blocks(X_val, tokenizer, max_tokens=hyperparameters['max_tokens'])\n",
    "    \n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    early_stopping = EarlyStopping(patience=hyperparameters['patience'],delta=0.001, verbose=True, alpha=0.5, min_combined_metric=0.65, lambda_penalty=0.1)\n",
    "\n",
    "    with open(log_file, \"a\") as log:  \n",
    "        for epoch in range(hyperparameters['n_epochs_max']):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for blocks, label in zip(train_blocks, y_train):\n",
    "                optimizer.zero_grad()\n",
    "                label_tensor = torch.tensor([label], dtype=torch.float).to(device)  \n",
    "                outputs = model(blocks)\n",
    "                loss = loss_fn(outputs.flatten(), label_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_blocks)\n",
    "            train_loss_history.append(train_loss)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            y_val_preds = []\n",
    "            y_val_true = []\n",
    "            with torch.no_grad():\n",
    "                for blocks, label in zip(val_blocks, y_val):\n",
    "                    label_tensor = torch.tensor([label], dtype=torch.float).to(device)\n",
    "                    outputs = model(blocks)\n",
    "                    loss = loss_fn(outputs.flatten(), label_tensor)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    y_val_preds.append(outputs.flatten().cpu().numpy())\n",
    "                    y_val_true.append(label.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(val_blocks)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            y_val_preds = np.concatenate(y_val_preds)\n",
    "            y_val_true = np.array(y_val_true)\n",
    "            accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp = calculate_metrics(y_val_true, y_val_preds)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Validation Metrics - Accuracy: {accuracy:.4f}, f1: {f1:.4f}, sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "            print(f\"Confusion Matrix - TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "\n",
    "            early_stopping(roc_auc, sensitivity, specificity, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break  \n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt')) \n",
    "\n",
    "    history = {'loss': train_loss_history, 'val_loss': val_loss_history}\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831aec69-92e7-493e-90e6-7b7067c29cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "   \n",
    "    Returns:\n",
    "        - metric_dev: Best validation loss achieved during training.     \n",
    "    \"\"\"\n",
    "    trial_seed = seed + trial.number  \n",
    "    set_seed(trial_seed)\n",
    "\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy['lr'] = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    hyperparameters_copy['dropout'] = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int(\"patience\", 3, 20)\n",
    "\n",
    "    v_metric_combined = [] \n",
    "    \n",
    "    model, history = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            trial_seed, \n",
    "        )\n",
    "\n",
    "    val_auc = np.max(hist.history[\"val_auc\"])\n",
    "    val_recall = np.max(hist.history[\"val_recall\"])  \n",
    "    val_specificity = np.max(hist.history[\"val_specificity\"]) \n",
    "    metric_combined = (0.5 * val_recall) + (0.5 * val_specificity) - (0.1 * abs(val_recall - val_specificity))\n",
    "    v_metric_combined.append(metric_combined)\n",
    "\n",
    "    metric_dev = np.mean(v_metric_combined)\n",
    "    return metric_dev\n",
    "\n",
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"  \n",
    "    set_seed(seed)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val),\n",
    "        n_trials=20,  \n",
    "        n_jobs=1      \n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "        'lr': best_params['lr'], \n",
    "        'weight_decay': best_params['weight_decay'], \n",
    "        'dropout': best_params['dropout'],  \n",
    "        'patience': best_params['patience'],  \n",
    "    }\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858de048-ee44-4ff7-af06-256fd117412a",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57be194-40d1-4871-b6aa-67fe2cd67028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "run_model = True\n",
    "results = []\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    " \n",
    "    y_pred_by_split = {}\n",
    "    bestHyperparameters_bySplit = {}\n",
    "        \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w14days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w14days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w14days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    " \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "        device = torch.device('cuda:0')\n",
    "        print(device)\n",
    "        loss_fn = AUCSensitivitySpecificityLoss(lambda_balance=0.5)\n",
    "\n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val\n",
    "        )\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "        \n",
    "        split_directory = f'./Results_GPT2/split_{i}'\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "        \n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            'lr': bestHyperparameters['lr'], \n",
    "            'weight_decay': bestHyperparameters['weight_decay'],\n",
    "            'dropout': bestHyperparameters['dropout'], \n",
    "            'patience': bestHyperparameters['patience']\n",
    "        })\n",
    "                    \n",
    "        \n",
    "        model, history = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    " \n",
    "        v_models.append(model)\n",
    "        loss_train.append(history['loss'])\n",
    "        loss_dev.append(history['val_loss'])\n",
    " \n",
    "        tokenizer = model.tokenizer\n",
    "        test_blocks = split_series_to_blocks(X_test, tokenizer, max_tokens=hyperparameters['max_tokens'])\n",
    " \n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for blocks in test_blocks:\n",
    "                outputs = model(blocks).detach().cpu().numpy()\n",
    "                y_pred.append(outputs)\n",
    "        y_pred = np.concatenate(y_pred)  \n",
    " \n",
    " \n",
    "        accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp = calculate_metrics(y_test, y_pred)\n",
    "        print(f\"Test Metrics for Split {i} - Accuracy: {accuracy:.4f}, f1: {f1:.4f}, sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix - TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    " \n",
    "        results.append([accuracy, sensitivity, specificity, roc_auc, f1])\n",
    " \n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        print(f\"for split {i}:\")\n",
    "        print(y_pred_by_split[str(i)]), \n",
    " \n",
    "        y_pred_path = os.path.join(split_directory, f\"y_pred_split_{i}.pkl\")\n",
    "        with open(y_pred_path, 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40db7a5-0ae4-4e9b-9edf-d35e2f47bb65",
   "metadata": {},
   "source": [
    "### RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3aea2-81d9-44fe-8f34-439b7fac4e7c",
   "metadata": {},
   "source": [
    "#### Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bef33-d9d5-4efb-925d-23014e47d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_GPT2'\n",
    "\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_GPT2/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_GPT2/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_GPT2/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389cd9c-0cb7-4209-82f8-cc8e8942c9fc",
   "metadata": {},
   "source": [
    "#### Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d52dda-9917-4a69-871c-1a748f16b15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f71952-972f-4898-9464-bedd858b8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GPT2 = pd.concat(all_metrics)\n",
    "metrics_GPT2.to_csv('./Results_GPT2/metrics_GPT2.csv', index=False)\n",
    "\n",
    "metrics_GPT2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a05779-d42b-4715-91e3-85b3ed69e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_GPT2.mean()\n",
    "metrics_std = metrics_GPT2.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "#summary_df.to_csv('./Results_GPT2/metrics_summary_GPT2.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e039882-52fd-4001-b4d9-4dd1f6a932ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GPT2 = pd.read_csv('./Results_GPT2/metrics_GPT2.csv')\n",
    "\n",
    "stats_GPT2 = metrics_GPT2.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_GPT2.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_GPT2/metrics_GPT2_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
