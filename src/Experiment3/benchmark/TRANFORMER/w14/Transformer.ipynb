{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, Dropout, Dense, Dropout, Flatten, Conv1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import utils_models \n",
    "import utils_interpretability\n",
    "import utils\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb24add-3925-44e8-b297-109f0ccd6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_sensitivity_specificity_loss(lambda_balance=1.0):\n",
    "    \"\"\"\n",
    "    Custom loss function that maximizes ROC-AUC and balances sensitivity and specificity,\n",
    "    while also handling masked values in y_true.\n",
    "\n",
    "    Args:\n",
    "        lambda_balance (float): Weight for balancing sensitivity-specificity trade-off.\n",
    "\n",
    "    Returns:\n",
    "        A Keras-compatible loss function.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred_probs):\n",
    "        mask = tf.not_equal(y_true, 666) \n",
    "        y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "        y_pred_probs_masked = tf.boolean_mask(y_pred_probs, mask)\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_probs_masked)\n",
    "\n",
    "        y_pred_labels = K.cast(y_pred_probs_masked > 0.5, dtype='float32')\n",
    "        y_true_masked = K.cast(y_true_masked, dtype='float32')  \n",
    "\n",
    "        tp = K.sum(y_pred_labels * y_true_masked)\n",
    "        tn = K.sum((1 - y_pred_labels) * (1 - y_true_masked))\n",
    "        fp = K.sum(y_pred_labels * (1 - y_true_masked))\n",
    "        fn = K.sum((1 - y_pred_labels) * y_true_masked)\n",
    "\n",
    "        sensitivity = tp / (tp + fn + K.epsilon())\n",
    "        specificity = tn / (tn + fp + K.epsilon())\n",
    "\n",
    "        balance_penalty = K.abs(sensitivity - specificity)\n",
    "        total_loss = bce * (1 + lambda_balance * balance_penalty)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e973f-b30a-4735-9c02-646bceebac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661302b5-8a48-4368-a20c-ff82296338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping(Callback):\n",
    "    def __init__(self, patience=10, delta=0.001, alpha=0.5, min_combined_metric=0.6, lambda_penalty=0.1, save_best_model=False):\n",
    "        \"\"\"\n",
    "        Early stops training if validation AUC does not improve after patience epochs \n",
    "        and recall & specificity are balanced.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait before stopping.\n",
    "            delta (float): Minimum improvement in AUC required to reset patience.\n",
    "            alpha (float): Weight for recall vs specificity trade-off.\n",
    "            min_combined_metric (float): Minimum threshold for combined recall-specificity metric.\n",
    "            lambda_penalty (float): Penalty for unbalanced recall and specificity.\n",
    "            save_best_model (bool): Whether to save the best model during training.\n",
    "        \"\"\"\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.alpha = alpha\n",
    "        self.min_combined_metric = min_combined_metric\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.best_auc = None\n",
    "        self.counter = 0\n",
    "        self.save_best_model = save_best_model\n",
    "        self.best_weights = None  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        val_auc = logs.get('val_auc')  \n",
    "        val_recall = logs.get('val_recall')  \n",
    "        val_specificity = logs.get('val_specificity') \n",
    "\n",
    "        if val_auc is None or val_recall is None or val_specificity is None:\n",
    "            print(\"Warning: Metrics missing for CustomEarlyStopping\")\n",
    "            return\n",
    "\n",
    "        metric_score = (self.alpha * val_recall) + ((1 - self.alpha) * val_specificity) - self.lambda_penalty * abs(val_recall - val_specificity)\n",
    "\n",
    "        if metric_score >= self.min_combined_metric:\n",
    "            if self.best_auc is None or val_auc > self.best_auc + self.delta:\n",
    "                self.best_auc = val_auc\n",
    "                self.counter = 0\n",
    "                print(f\"Epoch {epoch+1}: Model improved. Counter reset to 0.\")\n",
    "                if self.save_best_model:\n",
    "                    self.best_weights = self.model.get_weights()  # Guarda los mejores pesos\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                print(f\"Epoch {epoch+1}: No improvement. Counter {self.counter}/{self.patience}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"Epoch {epoch+1}: Metric score too low. Counter {self.counter}/{self.patience}\")\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "            if self.save_best_model and self.best_weights is not None:\n",
    "                print(\"Restoring best model weights.\")\n",
    "                self.model.set_weights(self.best_weights)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b658ca",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a Transformer model based on the provided training data and hyperparameters.\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the model hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CONSIDER THE HYPERPARAMETERS\n",
    "    hyperparameters['layers'] = [87, hyperparameters['middle_layer_dim'], 1]\n",
    "    dropout = hyperparameters[\"dropout\"]\n",
    "    num_heads = hyperparameters[\"num_heads\"]\n",
    "    num_transformer_blocks = hyperparameters[\"num_transformer_blocks\"]\n",
    "    activation = hyperparameters['activation']\n",
    "    l2_reg = hyperparameters.get('l2_reg',  1e-4)  # Add L2 regularization strength\n",
    "    optimizer = Adam(learning_rate=hyperparameters[\"lr_scheduler\"], weight_decay=hyperparameters[\"weight_decay\"])\n",
    "\n",
    "\n",
    "    input = Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    x = input\n",
    "    masked = x\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # NORMALIZATION AND ATTENTION\n",
    "        x_norm = layers.LayerNormalization()(masked)\n",
    "        x_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input.shape[-1])(x_norm, x_norm)\n",
    "        x_att_drop = layers.Dropout(dropout)(x_att)\n",
    "        res = x_att_drop + masked\n",
    "\n",
    "        # FEED FORWARD PART\n",
    "        x_ffn_norm = layers.LayerNormalization()(res)\n",
    "        x_ffn = layers.Dense(\n",
    "            input.shape[-1], activation=activation, kernel_regularizer=regularizers.l2(l2_reg)\n",
    "        )(x_ffn_norm)\n",
    "        x = x_ffn + res\n",
    "\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(\n",
    "        hyperparameters[\"layers\"][1], activation=activation, kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    output = layers.GlobalMaxPooling1D()(x)  \n",
    "    output = layers.Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(l2_reg))(output)  # Output: (None, 1)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    # COMPILE \n",
    "    model.compile(\n",
    "    loss=auc_sensitivity_specificity_loss(lambda_balance=0.5), \n",
    "    optimizer=optimizer,\n",
    "     metrics=[\n",
    "        'accuracy',  \n",
    "        tf.keras.metrics.AUC(name=\"auc\"),  \n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.SpecificityAtSensitivity(0.5, name=\"specificity\")]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, \n",
    "                hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built Transformer model based on the provided data and hyperparameters.\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.\n",
    "    Returns:\n",
    "        - model (tf.keras.Model): The trained Keras model.\n",
    "        - hist (tf.keras.callbacks.History): Training history object containing loss and metrics.\n",
    "        - training_time (float): Total training time in seconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the transformer\n",
    "    model = None\n",
    "    model = build_model(hyperparameters) \n",
    "\n",
    "    earlystopping = CustomEarlyStopping(\n",
    "            patience=hyperparameters[\"patience\"],\n",
    "            delta=hyperparameters[\"mindelta\"],\n",
    "            alpha=0.5,  \n",
    "            min_combined_metric=0.65,  \n",
    "            lambda_penalty=0.1,\n",
    "            save_best_model=True  \n",
    "    )\n",
    "    \n",
    "    X_val = tf.cast(X_val, tf.float64)\n",
    "    y_val = tf.cast(y_val, tf.float64)\n",
    "    X_train = tf.cast(X_train, tf.float64)\n",
    "    y_train = tf.cast(y_train, tf.float64)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopping],\n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        epochs=hyperparameters['n_epochs_max'],\n",
    "        verbose=hyperparameters[\"verbose\"],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, hist, training_time\n",
    "\n",
    "\n",
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.    \n",
    "    Returns:\n",
    "        - metric_dev (float): Best validation loss achieved during training.     \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Trial {trial.number} started\")\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    middle_dim = trial.suggest_int('middle_layer_dim', 2, 20, step=2)\n",
    "    hyperparameters_copy['middle_layer_dim'] = middle_dim\n",
    "    hyperparameters_copy[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hyperparameters_copy['l2_reg'] = trial.suggest_loguniform('l2_reg', 1e-6, 1e-2)\n",
    "    hyperparameters_copy['num_transformer_blocks'] = trial.suggest_int(\"num_transformer_blocks\", 1, 5)\n",
    "    hyperparameters_copy['activation'] = trial.suggest_categorical(\"activation\", ['tanh', 'LeakyReLU'])\n",
    "    hyperparameters_copy['num_heads'] = trial.suggest_int(\"num_heads\", 2, 10)\n",
    "    hyperparameters_copy['epsilon'] = trial.suggest_categorical(\"epsilon\", [0.9, 0.5, 0.1])\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int('patience', 3, 20)\n",
    "    hyperparameters_copy['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-5)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "\n",
    "    hyperparameters_copy['batch_size'] = hyperparameters['batch_size']\n",
    "    hyperparameters_copy['n_epochs_max'] = hyperparameters['n_epochs_max']\n",
    "    \n",
    "    v_metric_combined = []\n",
    "\n",
    "    model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "    )\n",
    "\n",
    "    val_auc = np.max(hist.history[\"val_auc\"])\n",
    "    val_recall = np.max(hist.history[\"val_recall\"])  \n",
    "    val_specificity = np.max(hist.history[\"val_specificity\"]) \n",
    "    metric_combined = (0.5 * val_recall) + (0.5 * val_specificity) - (0.1 * abs(val_recall - val_specificity))\n",
    "    v_metric_combined.append(metric_combined)\n",
    "\n",
    "    metric_dev = np.mean(v_metric_combined)\n",
    "    return metric_dev\n",
    "\n",
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.       \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize') \n",
    "    study.optimize(lambda trial: objective(trial, hyperparameters, seed, X_train, y_train , X_val, y_val, split, norm, n_time_steps), n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    layers = [87, best_params['middle_layer_dim'], 1]\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "                'dropout': best_params['dropout'],\n",
    "                'middle_layer_dim': best_params['middle_layer_dim'],\n",
    "                'layers': layers,\n",
    "                'lr_scheduler': best_params['lr_scheduler'],\n",
    "                'l2_reg': best_params['l2_reg'],\n",
    "                'num_transformer_blocks': best_params['num_transformer_blocks'],\n",
    "                'activation': best_params['activation'],\n",
    "                'num_heads': best_params['num_heads'],\n",
    "                'epsilon': best_params['epsilon'],\n",
    "                'batch_size': hyperparameters['batch_size'],\n",
    "                'n_epochs_max': hyperparameters['n_epochs_max'],\n",
    "                'patience': best_params['patience'],\n",
    "                'mindelta': best_params['mindelta'],\n",
    "        \n",
    "                'weight_decay': best_params['weight_decay']\n",
    "            }\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "    \n",
    "    return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447021ef",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3c500",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **n_epochs_max**: Maximum number of epochs for training.\n",
    "- **l2_reg**: L2 regularization coefficient.\n",
    "- **dropout**: Dropout rates.\n",
    "- **lr_scheduler**: Learning rates.\n",
    "- **norm**: Type of normalization applied to the data.\n",
    "- **num_heads**: Number of attention heads in the multi-head attention mechanism.\n",
    "- **num_transformer_blocks**: Number of transformer blocks.\n",
    "- **epsilon**: Avoid zero division in the normalization layer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization on weights.\n",
    "- **middle_layer_dim**: Different configurations for the middle layer of the model.\n",
    "- **mindelta**: Minimum delta required to consider as an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]\n",
    "\n",
    "input_shape = 87\n",
    "n_time_steps = 14\n",
    "batch_size = 4\n",
    "n_epochs_max = 100\n",
    "\n",
    "\n",
    "adjustment_factor = [1] \n",
    "activation = ['tanh', 'LeakyReLU']\n",
    "norm = \"standardScaler\"\n",
    "patience = 3\n",
    "num_heads = 7\n",
    "num_transformer_blocks = [5]\n",
    "epsilon = [0.9, 0.5, 0.1]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": patience,\n",
    "    \"dropout\": 0.2,\n",
    "    \"verbose\": 1,\n",
    "    \"input_shape\": input_shape,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"num_transformer_blocks\": 0,\n",
    "    \"l2_reg\": 1e-4,\n",
    "    \"epsilon\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766690a0",
   "metadata": {},
   "source": [
    "# RUNNING AND TRYING ON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "    \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w14days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w14days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w14days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    "        \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "\n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val,\n",
    "            f\"s{i}\",\n",
    "            norm,\n",
    "            n_time_steps\n",
    "        )\n",
    "        \n",
    "        fin = time.time()\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_Transformer_optuna/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"], \n",
    "            \"l2_reg\": bestHyperparameters[\"l2_reg\"],\n",
    "            \"num_transformer_blocks\": bestHyperparameters[\"num_transformer_blocks\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"],\n",
    "            'num_heads': bestHyperparameters['num_heads'],\n",
    "            'epsilon': bestHyperparameters['epsilon'],\n",
    "            \"patience\": bestHyperparameters[\"patience\"], \n",
    "            \"weight_decay\": bestHyperparameters[\"weight_decay\"],\n",
    "            \"mindelta\": bestHyperparameters[\"mindelta\"],\n",
    "            \"middle_layer_dim\": bestHyperparameters[\"middle_layer_dim\"]\n",
    "        })\n",
    "\n",
    "        #--- TRY ON TEST -----------------------------------------------------------------------#\n",
    "\n",
    "        utils_models.reset_keras()\n",
    "\n",
    "        model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        \n",
    "        best_val_auc = np.max(hist.history[\"val_auc\"])\n",
    "        best_val_recall = np.max(hist.history[\"val_recall\"])\n",
    "        best_val_specificity = np.max(hist.history[\"val_specificity\"])\n",
    "        metric_combined = (0.5 * best_val_recall) + (0.5 * best_val_specificity) - (0.1 * abs(best_val_recall - best_val_specificity))\n",
    "        loss_dev.append(metric_combined)\n",
    "        \n",
    "        training_times.append(training_time)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "            \n",
    "        with open(os.path.join(split_directory, \"training_times.pkl\"), 'wb') as f:\n",
    "            pickle.dump(training_times, f)\n",
    "                    \n",
    "    # END EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d440b",
   "metadata": {},
   "source": [
    "# RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b785b",
   "metadata": {},
   "source": [
    "## Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1692d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_Transformer_optuna'\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_Transformer_optuna/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818650d",
   "metadata": {},
   "source": [
    "## Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d5ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]: \n",
    "    y_test = pd.read_csv(f\"../../../DATA/w14days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66867d5",
   "metadata": {},
   "source": [
    "## Save results (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer = pd.concat(all_metrics)\n",
    "metrics_Transformer.to_csv('./Results_Transformer_optuna/metrics_Transformer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b2397-f8df-4189-a9d8-e06c8f3cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f069d-664f-4d20-b251-44e776fe4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_Transformer.mean()\n",
    "metrics_std = metrics_Transformer.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "summary_df.to_csv('./Results_Transformer_optuna/metrics_summary_Transformer.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faecc5a-f631-4d47-b7ff-754dd55de734",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_Transformer = pd.read_csv('./Results_Transformer_optuna/metrics_Transformer.csv')\n",
    "stats_Transformer = metrics_Transformer.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_Transformer.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_Transformer_optuna/metrics_Transformer_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
