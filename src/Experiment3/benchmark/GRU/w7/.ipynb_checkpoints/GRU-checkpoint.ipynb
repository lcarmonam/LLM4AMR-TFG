{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31f2d8-461a-4b1b-b291-3c3398d5b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import utils_models \n",
    "import utils_interpretability\n",
    "import utils\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4faef-6d82-4384-86f5-f2c65d137158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_sensitivity_specificity_loss(lambda_balance=1.0):\n",
    "    \"\"\"\n",
    "    Custom loss function that maximizes ROC-AUC and balances sensitivity and specificity,\n",
    "    while also handling masked values in y_true.\n",
    "\n",
    "    Args:\n",
    "        lambda_balance (float): Weight for balancing sensitivity-specificity trade-off.\n",
    "\n",
    "    Returns:\n",
    "        A Keras-compatible loss function.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred_probs):\n",
    "        mask = tf.not_equal(y_true, 666) \n",
    "        y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "        y_pred_probs_masked = tf.boolean_mask(y_pred_probs, mask)\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_probs_masked)\n",
    "\n",
    "        y_pred_labels = K.cast(y_pred_probs_masked > 0.5, dtype='float32')\n",
    "        y_true_masked = K.cast(y_true_masked, dtype='float32')  \n",
    "\n",
    "        tp = K.sum(y_pred_labels * y_true_masked)\n",
    "        tn = K.sum((1 - y_pred_labels) * (1 - y_true_masked))\n",
    "        fp = K.sum(y_pred_labels * (1 - y_true_masked))\n",
    "        fn = K.sum((1 - y_pred_labels) * y_true_masked)\n",
    "\n",
    "        sensitivity = tp / (tp + fn + K.epsilon())\n",
    "        specificity = tn / (tn + fp + K.epsilon())\n",
    "\n",
    "        balance_penalty = K.abs(sensitivity - specificity)\n",
    "        total_loss = bce * (1 + lambda_balance * balance_penalty)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff05ea-2cdc-4555-b33b-0957a332dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "    y_pred = np.round(y_pred_probs).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred) \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "\n",
    "    return accuracy, sensitivity, specificity, roc_auc, f1, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd096e39-2f61-4d9d-9386-3268cbc77f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping(Callback):\n",
    "    def __init__(self, patience=10, delta=0.001, alpha=0.5, min_combined_metric=0.6, lambda_penalty=0.1, save_best_model=False):\n",
    "        \"\"\"\n",
    "        Early stops training if validation AUC does not improve after patience epochs \n",
    "        and recall & specificity are balanced.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait before stopping.\n",
    "            delta (float): Minimum improvement in AUC required to reset patience.\n",
    "            alpha (float): Weight for recall vs specificity trade-off.\n",
    "            min_combined_metric (float): Minimum threshold for combined recall-specificity metric.\n",
    "            lambda_penalty (float): Penalty for unbalanced recall and specificity.\n",
    "            save_best_model (bool): Whether to save the best model during training.\n",
    "        \"\"\"\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.alpha = alpha\n",
    "        self.min_combined_metric = min_combined_metric\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.best_auc = None\n",
    "        self.counter = 0\n",
    "        self.save_best_model = save_best_model\n",
    "        self.best_weights = None  \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        val_auc = logs.get('val_auc')  \n",
    "        val_recall = logs.get('val_recall')  \n",
    "        val_specificity = logs.get('val_specificity') \n",
    "\n",
    "        if val_auc is None or val_recall is None or val_specificity is None:\n",
    "            print(\"Warning: Metrics missing for CustomEarlyStopping\")\n",
    "            return\n",
    "\n",
    "        metric_score = (self.alpha * val_recall) + ((1 - self.alpha) * val_specificity) - self.lambda_penalty * abs(val_recall - val_specificity)\n",
    "\n",
    "        if metric_score >= self.min_combined_metric:\n",
    "            if self.best_auc is None or val_auc > self.best_auc + self.delta:\n",
    "                self.best_auc = val_auc\n",
    "                self.counter = 0\n",
    "                print(f\"Epoch {epoch+1}: Model improved. Counter reset to 0.\")\n",
    "                if self.save_best_model:\n",
    "                    self.best_weights = self.model.get_weights()  # Guarda los mejores pesos\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                print(f\"Epoch {epoch+1}: No improvement. Counter {self.counter}/{self.patience}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"Epoch {epoch+1}: Metric score too low. Counter {self.counter}/{self.patience}\")\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "            if self.save_best_model and self.best_weights is not None:\n",
    "                print(\"Restoring best model weights.\")\n",
    "                self.model.set_weights(self.best_weights)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e72dea-acfc-45a8-b140-8a1521095f87",
   "metadata": {},
   "source": [
    "### FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f6163-a4d3-4304-af75-c4898fc64c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a GRU model based on several hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the model hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    hyperparameters['layers'] = [87, hyperparameters['middle_layer_dim'], 1]\n",
    "    l2_lambda = hyperparameters.get(\"l2_lambda\", 1e-4)\n",
    "    \n",
    "    dynamic_input = tf.keras.layers.Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    masked = dynamic_input\n",
    "    optimizer = Adam(learning_rate=hyperparameters[\"lr_scheduler\"], weight_decay=hyperparameters[\"weight_decay\"])\n",
    "\n",
    "    gru_encoder = tf.keras.layers.GRU(\n",
    "        hyperparameters[\"layers\"][1],\n",
    "        dropout=hyperparameters['dropout'],\n",
    "        return_sequences=False,\n",
    "        activation=hyperparameters['activation'],\n",
    "        kernel_regularizer=regularizers.l2(l2_lambda),\n",
    "        use_bias=False\n",
    "    )(masked)\n",
    "\n",
    "    if hyperparameters['dropout'] > 0.0:\n",
    "        gru_encoder = tf.keras.layers.Dropout(hyperparameters['dropout'])(gru_encoder)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, use_bias=False, activation=\"sigmoid\",kernel_regularizer=regularizers.l2(l2_lambda))(gru_encoder)\n",
    "\n",
    "    model = tf.keras.Model(dynamic_input, output)\n",
    "    model.compile(\n",
    "    loss=auc_sensitivity_specificity_loss(lambda_balance=0.5), \n",
    "    optimizer=optimizer,\n",
    "     metrics=[\n",
    "        'accuracy',  \n",
    "        tf.keras.metrics.AUC(name=\"auc\"),  \n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.SpecificityAtSensitivity(0.5, name=\"specificity\")]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5e399-a66f-4328-bc92-a8db8f5e33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(X_train, X_val, y_train, y_val, \n",
    "                hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built GRU model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - sample_weights_train, sample_weights_val: numpy.ndarray. Weights for the T and V data to handle class imbalance.\n",
    "        - hyperparameters: Dictionary containing the hyperparameters.\n",
    "        - seed: Integer seed for reproducibility.\n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the trained model.\n",
    "        - hist:  The training history.\n",
    "        - earlystopping: The early stopping callback.\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    earlystopping = CustomEarlyStopping(\n",
    "            patience=hyperparameters[\"patience\"],\n",
    "            delta=hyperparameters[\"mindelta\"],\n",
    "            alpha=0.5,  \n",
    "            min_combined_metric=0.65,  \n",
    "            lambda_penalty=0.1,\n",
    "            save_best_model=True  \n",
    "    )\n",
    "    start_time = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     callbacks=[earlystopping], batch_size=hyperparameters['batch_size'], epochs=hyperparameters['n_epochs_max'],\n",
    "                     verbose=hyperparameters['verbose'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, hist, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea3a15-d2b1-4ed3-8e99-d5bdc1311db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    Args:\n",
    "        - trial (optuna.trial.Trial): Optuna trial object.\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.    \n",
    "    Returns:\n",
    "        - metric_dev (float): Best validation loss achieved during training.     \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Trial {trial.number} started\")\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "\n",
    "    hyperparameters_copy[\"dropout\"] = trial.suggest_float('dropout', 0.0, 0.3)\n",
    "    middle_dim = trial.suggest_int('middle_layer_dim', 2, 20, step=2)\n",
    "    hyperparameters_copy['middle_layer_dim'] = middle_dim\n",
    "    hyperparameters_copy[\"lr_scheduler\"] = trial.suggest_loguniform('lr_scheduler', 1e-3, 1e-1)\n",
    "    hyperparameters_copy[\"l2_lambda\"] = trial.suggest_loguniform('l2_lambda', 1e-6, 1e-2)\n",
    "    hyperparameters_copy[\"adjustment_factor\"] = trial.suggest_categorical('adjustment_factor', [1])\n",
    "    hyperparameters_copy[\"activation\"] = trial.suggest_categorical('activation', ['tanh', 'LeakyReLU'])\n",
    "    hyperparameters_copy['patience'] = trial.suggest_int('patience', 3, 20)\n",
    "    hyperparameters_copy['mindelta'] = trial.suggest_loguniform('mindelta', 1e-10, 1e-5)\n",
    "    hyperparameters_copy['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "    \n",
    "\n",
    "    hyperparameters_copy['batch_size'] = hyperparameters['batch_size']\n",
    "    hyperparameters_copy['n_epochs_max'] = hyperparameters['n_epochs_max']\n",
    "   \n",
    "    v_metric_combined = []\n",
    "        \n",
    "\n",
    "    model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "    )\n",
    "\n",
    "    val_auc = np.max(hist.history[\"val_auc\"])\n",
    "    val_recall = np.max(hist.history[\"val_recall\"])  \n",
    "    val_specificity = np.max(hist.history[\"val_specificity\"]) \n",
    "    metric_combined = (0.5 * val_recall) + (0.5 * val_specificity) - (0.1 * abs(val_recall - val_specificity))\n",
    "    v_metric_combined.append(metric_combined)\n",
    "\n",
    "    metric_dev = np.mean(v_metric_combined)\n",
    "    return metric_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f881e-b237-4631-b9a8-185c6cc4a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_study(hyperparameters, seed, X_train, y_train, X_val, y_val, split, norm, n_time_steps):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for the model.\n",
    "    \n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - hyperparameters: Dictionary containing training and model hyperparameters.\n",
    "        - seed: Random seed for reproducibility.  \n",
    "        - split: String indicating the data split.\n",
    "        - norm: String with the type of normalization applied to the data.\n",
    "        - n_time_steps: Number of time steps in the input.       \n",
    "    Returns:\n",
    "        - best_hyperparameters: Dictionary containing the best hyperparameters found \n",
    "          after the optimization process.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='maximize') \n",
    "    study.optimize(lambda trial: objective(trial, hyperparameters, seed, X_train, y_train , X_val, y_val, split, norm, n_time_steps), n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_metric = study.best_value\n",
    "    \n",
    "    layers = [87, best_params['middle_layer_dim'], 1]\n",
    "    \n",
    "    best_hyperparameters = {\n",
    "        'dropout': best_params['dropout'],\n",
    "        'middle_layer_dim': best_params['middle_layer_dim'],\n",
    "        'layers': layers,\n",
    "        'lr_scheduler': best_params['lr_scheduler'],\n",
    "        'l2_lambda': best_params['l2_lambda'],\n",
    "        'adjustment_factor': best_params['adjustment_factor'],\n",
    "        'activation': best_params['activation'],\n",
    "        'batch_size': hyperparameters['batch_size'],\n",
    "        'n_epochs_max': hyperparameters['n_epochs_max'],\n",
    "        'patience': best_params['patience'],\n",
    "        'mindelta': best_params['mindelta'],\n",
    "        'weight_decay': best_params['weight_decay']\n",
    "    }\n",
    "    \n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Validation Metric: {best_metric}\")\n",
    "\n",
    "    return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3d036-d378-4070-9fca-3b054b8c8bfd",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262f664-9875-42a4-bed8-16176fb765b4",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **norm**: Type of normalization applied to the data.\n",
    "- **dropout**: Dropout rate to prevent overfitting.\n",
    "- **l2_lambda**: L2 regularization coefficient.\n",
    "- **lr_scheduler**: Learning rate assigned to the optimizer.\n",
    "- **patience**: Number of epochs with no improvement before early stopping is triggered.\n",
    "- **weight_decay**: Weight decay for the optimizer to apply additional L2 regularization on weights.\n",
    "- **middle_layer_dim**: Different configurations for the middle layer of the model.\n",
    "- **mindelta**: Minimum delta required to consider as an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0a288-018e-4fad-bbf4-2be13135a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 76, 227]\n",
    "\n",
    "input_shape = 87\n",
    "n_time_steps = 7\n",
    "batch_size = 4\n",
    "n_epochs_max = 100\n",
    "\n",
    "adjustment_factor = [1]  \n",
    "activation = ['tanh', 'LeakyReLU']\n",
    "norm = \"standardScaler\"\n",
    "patience = 3  \n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": patience,\n",
    "    \"dropout\": 0.2,\n",
    "    \"l2_lambda\": 1e-4,\n",
    "    \"verbose\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dc37e-323b-486b-a0b7-a62d3f6d79b4",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de524373-d8ca-4bd8-8c36-af823583fa2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = True\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    training_times = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "    \n",
    "    for i in [1,2,3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(f\"../../../DATA/w7days/s{i}/X_test_tensor_standardScaler.npy\")\n",
    "        y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "\n",
    "        X_train = np.load(f\"../../../DATA/w7days/s{i}/X_train_tensor_standardScaler.npy\")\n",
    "        y_train = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_train_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    \n",
    "        X_val = np.load(f\"../../../DATA/w7days/s{i}/X_val_tensor_standardScaler.npy\")\n",
    "        y_val = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_val_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "   \n",
    "        X_train = np.where(X_train == 666, 0, X_train)\n",
    "        X_val = np.where(X_val == 666, 0, X_val)\n",
    "        X_test = np.where(X_test == 666, 0, X_test)\n",
    "\n",
    "        bestHyperparameters = optuna_study(\n",
    "            hyperparameters,\n",
    "            seeds[i-1],\n",
    "            X_train, y_train,  \n",
    "            X_val, y_val,\n",
    "            f\"s{i}\",\n",
    "            norm,\n",
    "            n_time_steps\n",
    "        )\n",
    "        print(f\"Best layers: {bestHyperparameters['layers']}\")\n",
    "        \n",
    "        fin = time.time()\n",
    "        \n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_GRU_optuna/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters.update({\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"l2_lambda\": bestHyperparameters[\"l2_lambda\"],\n",
    "            \"adjustment_factor\": bestHyperparameters[\"adjustment_factor\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"], \n",
    "            \"patience\": bestHyperparameters[\"patience\"], \n",
    "            \"weight_decay\": bestHyperparameters[\"weight_decay\"],\n",
    "            \"mindelta\": bestHyperparameters[\"mindelta\"],\n",
    "            \"middle_layer_dim\": bestHyperparameters[\"middle_layer_dim\"]\n",
    "        })\n",
    "        \n",
    "        # --- TRY ON TEST -----------------------------------------------------------------------\n",
    "        utils_models.reset_keras()\n",
    "        print(hyperparameters)\n",
    "   \n",
    "\n",
    "        model, hist, training_time = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train, \n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        \n",
    "        best_val_auc = np.max(hist.history[\"val_auc\"])\n",
    "        best_val_recall = np.max(hist.history[\"val_recall\"])\n",
    "        best_val_specificity = np.max(hist.history[\"val_specificity\"])\n",
    "        metric_combined = (0.5 * best_val_recall) + (0.5 * best_val_specificity) - (0.1 * abs(best_val_recall - best_val_specificity))\n",
    "        loss_dev.append(metric_combined)\n",
    "        \n",
    "        training_times.append(training_time)\n",
    "\n",
    "        y_pred = model.predict(x=X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred \n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        with open(os.path.join(split_directory, \"training_times.pkl\"), 'wb') as f:\n",
    "            pickle.dump(training_times, f)\n",
    "\n",
    "    # END EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5a323-fb8a-4699-8df8-37a7a3fb9147",
   "metadata": {},
   "source": [
    "### RESULTS (PERFORMANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa62657-8ccb-4622-baa6-3d87bb607713",
   "metadata": {},
   "source": [
    "## Step 1. Load model and best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e588296-98b8-4076-86f7-10727655e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_GRU_optuna'\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    \n",
    "y_pred_by_split = {}\n",
    "y_pred_by_split['1'] = load_from_pickle(os.path.join('./Results_GRU_optuna/split_1', \"y_pred_split_1.pkl\"))\n",
    "y_pred_by_split['2'] = load_from_pickle(os.path.join('./Results_GRU_optuna/split_2', \"y_pred_split_2.pkl\"))\n",
    "y_pred_by_split['3'] = load_from_pickle(os.path.join('./Results_GRU_optuna/split_3', \"y_pred_split_3.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32561a63-d376-4dcf-8d85-fde9e9c2ba73",
   "metadata": {},
   "source": [
    "## Step 2. Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ad3a0-30e8-4bf1-ac5c-6e562fc5a143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in [1,2,3]: \n",
    "    y_test = pd.read_csv(f\"../../../DATA/w7days/s{i}/y_test_tensor_standardScaler.csv\")[\"individualMRGerm_stac\"].values.astype(int)\n",
    "    y_test_single = y_test.flatten()  \n",
    "    y_test_pred = y_pred_by_split[str(i)].flatten()  \n",
    "    \n",
    "    df_metrics = utils.get_metrics_(y_test_single, (y_test_pred))\n",
    "    print(df_metrics)\n",
    "    utils.plot_metrics(df_metrics)\n",
    "    utils.plot_roc_curve(y_test_single, y_test_pred)\n",
    "\n",
    "    all_metrics.append(df_metrics)\n",
    "print(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54945d6-b965-4023-a26d-a120ff2349ef",
   "metadata": {},
   "source": [
    "## Save results (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7606-a935-46a7-a27b-9eac995df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GRU = pd.concat(all_metrics)\n",
    "metrics_GRU.to_csv('./Results_GRU_optuna/metrics_GRU.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41e6ea-d570-456e-8e02-a71bff88b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GRU.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7f362-b157-4c5a-8e94-7966ee8689ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean = metrics_GRU.mean()\n",
    "metrics_std = metrics_GRU.std()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_mean.index,\n",
    "    \"Mean\": metrics_mean.values,\n",
    "    \"Standard Deviation\": metrics_std.values\n",
    "})\n",
    "\n",
    "summary_df.to_csv('./Results_GRU_optuna/metrics_summary_GRU.csv', index=False)\n",
    "\n",
    "print(\"\\nMean and Standard Deviation of the Splits:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54a150-4d1e-4a7c-a754-9c192c96c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GRU = pd.read_csv('./Results_GRU_optuna/metrics_GRU.csv')\n",
    "stats_GRU = metrics_GRU.agg([\"mean\", \"std\"]) \n",
    "formatted_metrics = stats_GRU.apply(lambda x: f\"{x['mean']*100:.2f} ± {x['std']*100:.2f}\", axis=0)\n",
    "formatted_metrics_df = pd.DataFrame(formatted_metrics, columns=[\"Metrics (Mean ± Std)\"])\n",
    "formatted_metrics_df.to_csv('./Results_GRU_optuna/metrics_GRU_formatted.csv', index=True)\n",
    "print(formatted_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
